{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjBOCJZHeYpf"
      },
      "source": [
        "# Vaccine Development with Monte Carlo Policy Evaluation\n",
        "\n",
        "As the CEO of a biotech company which is considering the development of a new vaccine. Starting at phase 0 (state 0), the drug develpment can stay in the same state or advance to \"phase 1  with promising results\" (state 1) or advance to \"phase 1 with disappointing results\" (state 2), or fail completely (state 4). At phase 1, the drug can stay in the same state, fail or become a success (state 3), in which case you will sell its patent to a big pharma company for \\$10 million.\n",
        "These state transitions happen from month to month, and at each state, you have the option to make an additional investment of \\$100,000, which increases the chances of success.\n",
        "\n",
        "After careful study, your analysts develop the program below to simulate different scenarios using statistical data from similar projects. For this question, you don't need to understand what is inside the program, only how to use it. If you want, you can can skip to the next cell.\n",
        "\n",
        "Use the Monte Carlo method to find the value functions induced by the following policies:\n",
        "\n",
        "\n",
        "a) Always invest\n",
        "\n",
        "b) Never invest\n",
        "\n",
        "Use a discount factor of 0.9960 to discount future rewards.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnAvrShs6ecs"
      },
      "source": [
        "import numpy as np\n",
        "class MDP():\n",
        "  def __init__(self):\n",
        "    self.A = [0, 1]\n",
        "    self.S = [0, 1, 2, 3, 4]\n",
        "\n",
        "    P0 = np.array([[0.5, .15, .15, 0, .20],\n",
        "                   [0, .5, .0, .25, .25],\n",
        "                   [0, 0, .15, .05, .8],\n",
        "                   [0, 0, 0, 0, 1],\n",
        "                   [0, 0, 0, 0, 1]])\n",
        "\n",
        "    R0 = np.array([0, 0, 0, 10, 0])\n",
        "\n",
        "    P1 = np.array([[0.5, .25, .15, 0, .10],\n",
        "                   [0, .5, .0, .35, .15],\n",
        "                   [0, 0, .20, .05, .75],\n",
        "                   [0, 0, 0, 0, 1],\n",
        "                   [0, 0, 0, 0, 1]])\n",
        "\n",
        "    R1 = np.array([-0.1, -0.1, -0.1, 10, 0])\n",
        "\n",
        "    self.P = [P0, P1]\n",
        "    self.R = [R0, R1]\n",
        "\n",
        "  def step(self, s, a):\n",
        "    s_prime = np.random.choice(len(self.S), p=self.P[a][s])\n",
        "    R = self.R[a][s]\n",
        "    if s_prime == 4:\n",
        "      done = True\n",
        "    else:\n",
        "      done = False\n",
        "    return s_prime, R, done\n",
        "\n",
        "  def simulate(self, s, a, π):\n",
        "    done = False\n",
        "    t = 0\n",
        "    history = []\n",
        "    while not done:\n",
        "      if t > 0:\n",
        "        a = π[s]\n",
        "      s_prime, R, done = self.step(s, a)\n",
        "      history.append((s, a, R))\n",
        "      s = s_prime\n",
        "      t += 1\n",
        "\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-rfjh_37kmX"
      },
      "source": [
        "# Here is how you can use the program\n",
        "# First we create an instance of this MDP\n",
        "mdp = MDP()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRXv7TYB7h2_",
        "outputId": "cac7cde8-1b3f-4cdc-c324-2a96c8377d35"
      },
      "source": [
        "# Now, starting at phase 0 (state s=0), we will choose to make additional investments (a=1).\n",
        "s = 0\n",
        "a = 1\n",
        "\n",
        "π = [0, 0, 0, 0]\n",
        "\n",
        "# We feed the state-action pair to the simulator and it tells us what happened in this simulation:\n",
        "history = mdp.simulate(s, a, π)\n",
        "\n",
        "history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 1, -0.1)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWfK-47V8I08"
      },
      "source": [
        "# In the simulation above, we have 3 observations of the triplet (state, action, reward)\n",
        "# Notice the reward of -0.1. This is because we chose to make additional investments (the units are in millions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MDP():\n",
        "    def __init__(self):\n",
        "        self.A = [0, 1]  # Actions: no investment, invest\n",
        "        self.S = [0, 1, 2, 3, 4]  # States\n",
        "\n",
        "        # Transition probabilities without investment\n",
        "        P0 = np.array([\n",
        "            [0.5, 0.15, 0.15, 0, 0.20],\n",
        "            [0, 0.5, 0.0, 0.25, 0.25],\n",
        "            [0, 0, 0.15, 0.05, 0.8],\n",
        "            [0, 0, 0, 0, 1],\n",
        "            [0, 0, 0, 0, 1]\n",
        "        ])\n",
        "\n",
        "        # Rewards without investment\n",
        "        R0 = np.array([0, 0, 0, 10, 0])\n",
        "\n",
        "        # Transition probabilities with investment\n",
        "        P1 = np.array([\n",
        "            [0.5, 0.25, 0.15, 0, 0.10],\n",
        "            [0, 0.5, 0.0, 0.35, 0.15],\n",
        "            [0, 0, 0.20, 0.05, 0.75],\n",
        "            [0, 0, 0, 0, 1],\n",
        "            [0, 0, 0, 0, 1]\n",
        "        ])\n",
        "\n",
        "        # Rewards with investment (including -0.1 investment cost)\n",
        "        R1 = np.array([-0.1, -0.1, -0.1, 10, 0])\n",
        "\n",
        "        self.P = [P0, P1]\n",
        "        self.R = [R0, R1]\n",
        "\n",
        "    def step(self, s, a):\n",
        "        # Simulate next state and reward\n",
        "        s_prime = np.random.choice(len(self.S), p=self.P[a][s])\n",
        "        R = self.R[a][s]\n",
        "        done = (s_prime == 4)\n",
        "        return s_prime, R, done\n",
        "\n",
        "    def monte_carlo_evaluation(self, policy, num_episodes=10000, discount_factor=0.9960):\n",
        "        # Initialize value function estimate\n",
        "        V = np.zeros(len(self.S))\n",
        "        returns = [[] for _ in range(len(self.S))]\n",
        "\n",
        "        for _ in range(num_episodes):\n",
        "            # Start at initial state\n",
        "            s = 0\n",
        "            history = []\n",
        "            done = False\n",
        "\n",
        "            # Generate episode\n",
        "            while not done:\n",
        "                # Choose action based on policy\n",
        "                a = policy[s]\n",
        "\n",
        "                # Take step\n",
        "                s_prime, R, done = self.step(s, a)\n",
        "\n",
        "                # Store experience\n",
        "                history.append((s, a, R))\n",
        "                s = s_prime\n",
        "\n",
        "            # Calculate returns\n",
        "            G = 0\n",
        "            for t in range(len(history)-1, -1, -1):\n",
        "                s, a, R = history[t]\n",
        "                G = R + discount_factor * G\n",
        "                returns[s].append(G)\n",
        "\n",
        "        # Calculate average returns\n",
        "        for s in range(len(self.S)):\n",
        "            if returns[s]:\n",
        "                V[s] = np.mean(returns[s])\n",
        "\n",
        "        return V\n",
        "\n",
        "# Create MDP instance\n",
        "mdp = MDP()\n",
        "\n",
        "# Policy A: Always invest\n",
        "policy_always_invest = [1, 1, 1, 0, 0]\n",
        "V_always_invest = mdp.monte_carlo_evaluation(policy_always_invest)\n",
        "\n",
        "# Policy B: Never invest\n",
        "policy_never_invest = [0, 0, 0, 0, 0]\n",
        "V_never_invest = mdp.monte_carlo_evaluation(policy_never_invest)\n",
        "\n",
        "print(\"Value Function - Always Invest:\")\n",
        "for s, v in enumerate(V_always_invest):\n",
        "    print(f\"State {s}: {v:.4f}\")\n",
        "\n",
        "print(\"\\nValue Function - Never Invest:\")\n",
        "for s, v in enumerate(V_never_invest):\n",
        "    print(f\"State {s}: {v:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvzujwFwi76O",
        "outputId": "1c22e41b-687c-4c5d-d1c0-b8873211a0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Function - Always Invest:\n",
            "State 0: 3.4221\n",
            "State 1: 6.8745\n",
            "State 2: 0.5052\n",
            "State 3: 10.0000\n",
            "State 4: 0.0000\n",
            "\n",
            "Value Function - Never Invest:\n",
            "State 0: 1.6242\n",
            "State 1: 4.9826\n",
            "State 2: 0.5355\n",
            "State 3: 10.0000\n",
            "State 4: 0.0000\n"
          ]
        }
      ]
    }
  ]
}